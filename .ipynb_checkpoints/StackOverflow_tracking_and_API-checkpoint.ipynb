{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a1c6e-270d-428b-a198-f3615c64d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from time import time\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             hamming_loss, jaccard_score, confusion_matrix, roc_curve, auc)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795b899-97f7-4cd2-a221-11b49cdf9717",
   "metadata": {},
   "source": [
    "Elements du tracking : \n",
    "\n",
    "**1. Métriques de Performance** (Scores) :\n",
    "   \n",
    "<u>Accuracy</u> : Précision moyenne des tags prédits.  \n",
    "\n",
    "<u>Precision</u> (micro, macro, weighted) : Précision des prédictions, évaluée de plusieurs manières.  \n",
    "\n",
    "<u>Recall</u> (micro, macro, weighted) : Taux de rappel des prédictions.  \n",
    "\n",
    "<u>F1 Score</u> (micro, macro, weighted) : Moyenne harmonique de la précision et du rappel.  \n",
    "\n",
    "<u>Hamming Loss</u> : Nombre moyen d'étiquettes incorrectes par instance.  \n",
    "\n",
    "<u>Subset Accuracy</u> : Proportion d'instances pour lesquelles toutes les étiquettes prédictes sont correctes.  \n",
    "\n",
    "<u>Jaccard Similarity</u> : Mesure de la similarité entre les ensembles de tags prédits et réels.  \n",
    "  \n",
    "**2. Hyperparamètres**\n",
    "   \n",
    "<u>Paramètres du Modèle</u> :  \n",
    "Pour un Random Forest : n_estimators, max_depth, min_samples_split, min_samples_leaf, etc.  \n",
    "Pour un SVM : C, kernel, gamma, etc.  \n",
    "Pour un modèle de deep learning : learning_rate, batch_size, num_epochs, dropout_rate, etc.  \n",
    "\n",
    "<u>Prétraitement</u> : \n",
    "Taille du vocabulaire.  \n",
    "Méthodes de vectorisation (TF-IDF, embeddings).  \n",
    "Paramètres de tokenisation (n-grams, stop words).  \n",
    "\n",
    "\n",
    "**3. Artifacts**\n",
    "\n",
    "<u>Modèles Entraînés</u> : Modèles sauvegardés pour chaque run.    \n",
    "\n",
    "<u>Vecteurs de Features</u> : Vecteurs résultant de la vectorisation des questions. \n",
    "\n",
    "<u>Fichiers de Logs</u> : Logs de l'entraînement et de la validation.    \n",
    "\n",
    "<u>Matrice de Confusion</u> : Pour visualiser les prédictions correctes et incorrectes.  \n",
    "\n",
    "<u>Courbes ROC/AUC</u> : Pour les modèles supportant predict_proba.    \n",
    "\n",
    "<u>Courbes d'apprentissage</u> : Visualisation de la progression des métriques de performance pendant l'entraînement.    \n",
    "\n",
    "**4. Graphiques Intéressants à Visualiser**  \n",
    "\n",
    "<u>Courbes d'Apprentissage</u> : Visualiser l'évolution de la perte et des métriques (précision, rappel, F1) pendant l'entraînement.  \n",
    "\n",
    "<u>Matrice de Confusion</u> : Affichage de la matrice de confusion pour comprendre les erreurs de classification.  \n",
    "\n",
    "<u>Courbe ROC</u> : Affichage de la courbe ROC pour les modèles de classification probabilistes.  \n",
    "\n",
    "<u>Importance des Features</u> : Graphique montrant l'importance des différentes features pour les modèles comme les Random Forests.  \n",
    "\n",
    "<u>Distribution des Tags Prédits vs Réels</u> : Comparaison de la distribution des tags prédits par le modèle par rapport aux tags réels.  \n",
    "\n",
    "<u>Histogrammes des Temps de Traitement</u> : Visualisation des temps de traitement pour l'entraînement et la prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bba88b-6734-44fc-a882-7c52e681ed97",
   "metadata": {},
   "source": [
    "## Fonctions de tracking MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e76c1-d720-4975-acf1-2daa2972b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def track_classification_experiment(model, X_train, y_train, X_test, y_test, params, class_labels=None):\n",
    "    with mlflow.start_run():\n",
    "        # Enregistrement des hyperparamètres\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Entraînement du modèle\n",
    "        start_time = time()\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time() - start_time\n",
    "        mlflow.log_metric(\"training_time\", training_time)\n",
    "\n",
    "        # Prédictions et calcul des scores\n",
    "        start_time = time()\n",
    "        predictions = model.predict(X_test)\n",
    "        prediction_time = time() - start_time\n",
    "        mlflow.log_metric(\"prediction_time\", prediction_time)\n",
    "        \n",
    "        # Calcul des scores\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        prec_micro = precision_score(y_test, predictions, average='micro')\n",
    "        rec_micro = recall_score(y_test, predictions, average='micro')\n",
    "        f1_micro = f1_score(y_test, predictions, average='micro')\n",
    "        ham_loss = hamming_loss(y_test, predictions)\n",
    "        subset_acc = np.mean([set(pred) == set(real) for pred, real in zip(predictions, y_test)])\n",
    "        jaccard = jaccard_score(y_test, predictions, average='samples')\n",
    "\n",
    "        # Enregistrement des scores\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision_micro\", prec_micro)\n",
    "        mlflow.log_metric(\"recall_micro\", rec_micro)\n",
    "        mlflow.log_metric(\"f1_score_micro\", f1_micro)\n",
    "        mlflow.log_metric(\"hamming_loss\", ham_loss)\n",
    "        mlflow.log_metric(\"subset_accuracy\", subset_acc)\n",
    "        mlflow.log_metric(\"jaccard_similarity\", jaccard)\n",
    "\n",
    "        # Enregistrement de la matrice de confusion\n",
    "        if class_labels is not None:\n",
    "            cm = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
    "            plt.figure()\n",
    "            plot_confusion_matrix(cm, classes=class_labels, title='Confusion matrix')\n",
    "            plt.savefig('confusion_matrix.png')\n",
    "            mlflow.log_artifact('confusion_matrix.png')\n",
    "\n",
    "        # Enregistrement des courbes ROC (exemple pour les modèles supportant predict_proba)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probas = model.predict_proba(X_test)\n",
    "            for i, class_label in enumerate(class_labels):\n",
    "                fpr, tpr, thresholds = roc_curve(y_test[:, i], probas[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'ROC Curve for {class_label}')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(f'roc_curve_{class_label}.png')\n",
    "                mlflow.log_artifact(f'roc_curve_{class_label}.png')\n",
    "\n",
    "        # Enregistrement du modèle\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        # Enregistrement de l'environnement\n",
    "        mlflow.set_tag(\"mlflow_version\", mlflow.__version__)\n",
    "        mlflow.set_tag(\"model_type\", type(model).__name__)\n",
    "        # Log des versions des packages utilisés (sklearn, numpy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af455d-5486-481c-a27d-119748d4a8be",
   "metadata": {},
   "source": [
    "### Exemple d'utilisation des fonctions de tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211b553-c13f-4006-93e7-08b290c166ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exemple d'utilisation\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# # Exemple d'initialisation des données\n",
    "# X_train = ...  # Vos données d'entraînement\n",
    "# X_test = ...   # Vos données de test\n",
    "# y_train = ...  # Vos étiquettes d'entraînement\n",
    "# y_test = ...   # Vos étiquettes de test\n",
    "\n",
    "# model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, max_depth=10))\n",
    "# params = {\"n_estimators\": 100, \"max_depth\": 10}\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# y_train_bin = mlb.fit_transform(y_train)\n",
    "# y_test_bin = mlb.transform(y_test)\n",
    "\n",
    "# class_labels = mlb.classes_\n",
    "# track_classification_experiment(model, X_train, y_train_bin, X_test, y_test_bin, params, class_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
