{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33437d39-4295-4665-99d6-ff258f4c87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maudt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maudt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.output_scroll { height: auto !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>link</th>\n",
       "      <th>view_count</th>\n",
       "      <th>score</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is processing a sorted array faster than p...</td>\n",
       "      <td>&lt;p&gt;In this C++ code, sorting the data (&lt;em&gt;bef...</td>\n",
       "      <td>https://stackoverflow.com/questions/11227809/w...</td>\n",
       "      <td>1894410</td>\n",
       "      <td>27300</td>\n",
       "      <td>java,c++,performance,cpu-architecture,branch-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I delete a Git branch locally and remot...</td>\n",
       "      <td>&lt;p&gt;Failed Attempts to Delete a Remote Branch:&lt;...</td>\n",
       "      <td>https://stackoverflow.com/questions/2003505/ho...</td>\n",
       "      <td>11766932</td>\n",
       "      <td>20374</td>\n",
       "      <td>git,version-control,git-branch,git-push,git-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the &amp;#39;--&amp;gt;&amp;#39; operator in C/C++?</td>\n",
       "      <td>&lt;p&gt;After reading &lt;a href=\"http://groups.google...</td>\n",
       "      <td>https://stackoverflow.com/questions/1642028/wh...</td>\n",
       "      <td>1018216</td>\n",
       "      <td>10170</td>\n",
       "      <td>c++,c,operators,code-formatting,standards-comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I force &amp;quot;git pull&amp;quot; to overwri...</td>\n",
       "      <td>&lt;p&gt;How do I force an overwrite of local files ...</td>\n",
       "      <td>https://stackoverflow.com/questions/1125968/ho...</td>\n",
       "      <td>8726770</td>\n",
       "      <td>9730</td>\n",
       "      <td>git,version-control,overwrite,git-pull,git-fetch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What and where are the stack and heap?</td>\n",
       "      <td>&lt;ul&gt;\\n&lt;li&gt;What are the stack and heap?&lt;/li&gt;\\n&lt;...</td>\n",
       "      <td>https://stackoverflow.com/questions/79923/what...</td>\n",
       "      <td>1950792</td>\n",
       "      <td>9450</td>\n",
       "      <td>data-structures,memory-management,heap-memory,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Why is processing a sorted array faster than p...   \n",
       "1  How do I delete a Git branch locally and remot...   \n",
       "2    What is the &#39;--&gt;&#39; operator in C/C++?   \n",
       "3  How do I force &quot;git pull&quot; to overwri...   \n",
       "4             What and where are the stack and heap?   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>In this C++ code, sorting the data (<em>bef...   \n",
       "1  <p>Failed Attempts to Delete a Remote Branch:<...   \n",
       "2  <p>After reading <a href=\"http://groups.google...   \n",
       "3  <p>How do I force an overwrite of local files ...   \n",
       "4  <ul>\\n<li>What are the stack and heap?</li>\\n<...   \n",
       "\n",
       "                                                link  view_count  score  \\\n",
       "0  https://stackoverflow.com/questions/11227809/w...     1894410  27300   \n",
       "1  https://stackoverflow.com/questions/2003505/ho...    11766932  20374   \n",
       "2  https://stackoverflow.com/questions/1642028/wh...     1018216  10170   \n",
       "3  https://stackoverflow.com/questions/1125968/ho...     8726770   9730   \n",
       "4  https://stackoverflow.com/questions/79923/what...     1950792   9450   \n",
       "\n",
       "                                                tags  \n",
       "0  java,c++,performance,cpu-architecture,branch-p...  \n",
       "1  git,version-control,git-branch,git-push,git-re...  \n",
       "2  c++,c,operators,code-formatting,standards-comp...  \n",
       "3   git,version-control,overwrite,git-pull,git-fetch  \n",
       "4  data-structures,memory-management,heap-memory,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#tracking mlflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from time import time\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,hamming_loss, jaccard_score, confusion_matrix, roc_curve, auc)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Prétraitement\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Extraction de features\n",
    "#Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#Word2Vec\n",
    "#from gensim.models import Word2Vec\n",
    "#BERT\n",
    "#from transformers import BertTokenizer, BertModel\n",
    "#import torch\n",
    "#USE\n",
    "#import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#empecher les messages d'erreur\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Formatage taille cellule pour s'adapter aux graphiques\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.output_scroll { height: auto !important; }</style>\"))\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./filtered_questions.csv\",sep=',', encoding='utf-8')  \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff582f4d-36cf-42bf-9604-c7d7e4d188ae",
   "metadata": {},
   "source": [
    "## Tracking MLFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ef01-b6c4-44b8-b6e9-5f75cb29b116",
   "metadata": {},
   "source": [
    "Elements du tracking :\n",
    "\n",
    "1. Métriques de Performance (Scores) :\n",
    "\n",
    "Accuracy : Précision moyenne des tags prédits.\n",
    "\n",
    "Precision (micro, macro, weighted) : Précision des prédictions, évaluée de plusieurs manières.\n",
    "\n",
    "Recall (micro, macro, weighted) : Taux de rappel des prédictions.\n",
    "\n",
    "F1 Score (micro, macro, weighted) : Moyenne harmonique de la précision et du rappel.\n",
    "\n",
    "Hamming Loss : Nombre moyen d'étiquettes incorrectes par instance.\n",
    "\n",
    "Subset Accuracy : Proportion d'instances pour lesquelles toutes les étiquettes prédictes sont correctes.\n",
    "\n",
    "Jaccard Similarity : Mesure de la similarité entre les ensembles de tags prédits et réels.\n",
    "\n",
    "2. Hyperparamètres\n",
    "\n",
    "Paramètres du Modèle :\n",
    "Pour un Random Forest : n_estimators, max_depth, min_samples_split, min_samples_leaf, etc.\n",
    "Pour un SVM : C, kernel, gamma, etc.\n",
    "Pour un modèle de deep learning : learning_rate, batch_size, num_epochs, dropout_rate, etc.\n",
    "\n",
    "Prétraitement : Taille du vocabulaire.\n",
    "Méthodes de vectorisation (TF-IDF, embeddings).\n",
    "Paramètres de tokenisation (n-grams, stop words).\n",
    "\n",
    "3. Artifacts\n",
    "\n",
    "Modèles Entraînés : Modèles sauvegardés pour chaque run.\n",
    "\n",
    "Vecteurs de Features : Vecteurs résultant de la vectorisation des questions.\n",
    "\n",
    "Fichiers de Logs : Logs de l'entraînement et de la validation.\n",
    "\n",
    "Matrice de Confusion : Pour visualiser les prédictions correctes et incorrectes.\n",
    "\n",
    "Courbes ROC/AUC : Pour les modèles supportant predict_proba.\n",
    "\n",
    "Courbes d'apprentissage : Visualisation de la progression des métriques de performance pendant l'entraînement.\n",
    "\n",
    "4. Graphiques Intéressants à Visualiser\n",
    "\n",
    "Courbes d'Apprentissage : Visualiser l'évolution de la perte et des métriques (précision, rappel, F1) pendant l'entraînement.\n",
    "\n",
    "Matrice de Confusion : Affichage de la matrice de confusion pour comprendre les erreurs de classification.\n",
    "\n",
    "Courbe ROC : Affichage de la courbe ROC pour les modèles de classification probabilistes.\n",
    "\n",
    "Importance des Features : Graphique montrant l'importance des différentes features pour les modèles comme les Random Forests.\n",
    "\n",
    "Distribution des Tags Prédits vs Réels : Comparaison de la distribution des tags prédits par le modèle par rapport aux tags réels.\n",
    "\n",
    "Histogrammes des Temps de Traitement : Visualisation des temps de traitement pour l'entraînement et la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db465fe-3395-42af-a102-8806eb9991b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def track_classification_experiment(model, X_train, y_train, X_test, y_test, params, class_labels=None):\n",
    "    with mlflow.start_run():\n",
    "        # Enregistrement des hyperparamètres\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Entraînement du modèle\n",
    "        start_time = time()\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time() - start_time\n",
    "        mlflow.log_metric(\"training_time\", training_time)\n",
    "\n",
    "        # Prédictions et calcul des scores\n",
    "        start_time = time()\n",
    "        predictions = model.predict(X_test)\n",
    "        prediction_time = time() - start_time\n",
    "        mlflow.log_metric(\"prediction_time\", prediction_time)\n",
    "        \n",
    "        # Calcul des scores\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        prec_micro = precision_score(y_test, predictions, average='micro')\n",
    "        rec_micro = recall_score(y_test, predictions, average='micro')\n",
    "        f1_micro = f1_score(y_test, predictions, average='micro')\n",
    "        ham_loss = hamming_loss(y_test, predictions)\n",
    "        subset_acc = np.mean([set(pred) == set(real) for pred, real in zip(predictions, y_test)])\n",
    "        jaccard = jaccard_score(y_test, predictions, average='samples')\n",
    "\n",
    "        # Enregistrement des scores\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision_micro\", prec_micro)\n",
    "        mlflow.log_metric(\"recall_micro\", rec_micro)\n",
    "        mlflow.log_metric(\"f1_score_micro\", f1_micro)\n",
    "        mlflow.log_metric(\"hamming_loss\", ham_loss)\n",
    "        mlflow.log_metric(\"subset_accuracy\", subset_acc)\n",
    "        mlflow.log_metric(\"jaccard_similarity\", jaccard)\n",
    "\n",
    "        # Enregistrement de la matrice de confusion\n",
    "        if class_labels is not None:\n",
    "            cm = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
    "            plt.figure()\n",
    "            plot_confusion_matrix(cm, classes=class_labels, title='Confusion matrix')\n",
    "            plt.savefig('confusion_matrix.png')\n",
    "            mlflow.log_artifact('confusion_matrix.png')\n",
    "\n",
    "        # Enregistrement des courbes ROC (exemple pour les modèles supportant predict_proba)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probas = model.predict_proba(X_test)\n",
    "            for i, class_label in enumerate(class_labels):\n",
    "                fpr, tpr, thresholds = roc_curve(y_test[:, i], probas[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'ROC Curve for {class_label}')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(f'roc_curve_{class_label}.png')\n",
    "                mlflow.log_artifact(f'roc_curve_{class_label}.png')\n",
    "\n",
    "        # Enregistrement du modèle\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        # Enregistrement de l'environnement\n",
    "        mlflow.set_tag(\"mlflow_version\", mlflow.__version__)\n",
    "        mlflow.set_tag(\"model_type\", type(model).__name__)\n",
    "        # Log des versions des packages utilisés (sklearn, numpy, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1516df-0c63-4abe-bc2d-7a1231be2e4a",
   "metadata": {},
   "source": [
    "### Exemple d'utilisation des fonctions de tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0adb0-cfac-4414-b3c3-7c8e6fc5ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exemple d'utilisation\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# # Exemple d'initialisation des données\n",
    "# X_train = ...  # Vos données d'entraînement\n",
    "# X_test = ...   # Vos données de test\n",
    "# y_train = ...  # Vos étiquettes d'entraînement\n",
    "# y_test = ...   # Vos étiquettes de test\n",
    "\n",
    "# model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, max_depth=10))\n",
    "# params = {\"n_estimators\": 100, \"max_depth\": 10}\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# y_train_bin = mlb.fit_transform(y_train)\n",
    "# y_test_bin = mlb.transform(y_test)\n",
    "\n",
    "# class_labels = mlb.classes_\n",
    "# track_classification_experiment(model, X_train, y_train_bin, X_test, y_test_bin, params, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9945d-0e83-4faa-aff0-3bbda3ba73c2",
   "metadata": {},
   "source": [
    "## Prétraitement de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "883f6a5e-8258-4831-8d8e-b4494d0246b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    processing sorted array faster processing unso...\n",
       "1                   delete git branch locally remotely\n",
       "2                                         operator c c\n",
       "3                  force git pull overwrite local file\n",
       "4                                           stack heap\n",
       "Name: clean_title, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Supprimer les balises HTML\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Supprimer la ponctuation\n",
    "    words = text.lower().split()  # Mettre en minuscule et diviser en mots\n",
    "    stop_words = set(stopwords.words('english'))  # Obtenir les stopwords anglais\n",
    "    words = [w for w in words if w not in stop_words]  # Supprimer les stopwords\n",
    "    lemmatizer = WordNetLemmatizer()  # Initialiser le lemmatizer\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]  # Lemmatization\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Appliquer le nettoyage aux colonnes 'title' et 'body'\n",
    "df['clean_title'] = df['title'].apply(clean_text)\n",
    "df['clean_body'] = df['body'].apply(clean_text)\n",
    "\n",
    "# Séparer les tags en liste\n",
    "df['tags'] = df['tags'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Binariser les tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_tags = mlb.fit_transform(df['tags'])\n",
    "\n",
    "# Séparer les données en jeux d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['clean_title', 'clean_body']], df_tags, test_size=0.2, random_state=42)\n",
    "\n",
    "# Afficher un aperçu des données nettoyées\n",
    "df['clean_title'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65911110-89ea-4643-9a80-39c0b2ce5b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    c code sorting data timed region make primary ...\n",
       "1    failed attempt delete remote branch git branch...\n",
       "2    reading hidden feature dark corner c stl comp ...\n",
       "3    force overwrite local file git pull local repo...\n",
       "4    stack heap located physically computer memory ...\n",
       "Name: clean_body, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_body'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebefab09-1767-4c53-b4ab-41a9314c451f",
   "metadata": {},
   "source": [
    "## Extraction de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b954fb-9a42-49d6-a859-9dc27ae4cf4f",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e95424ba-d0f1-45d0-9178-4935a93409a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Créer le CountVectorizer pour 'title'\n",
    "# count_vect = CountVectorizer()\n",
    "# count_vect.fit(X_train['clean_title'])\n",
    "\n",
    "# # Transformer 'title' et 'body' des jeux d'entraînement et de test\n",
    "# X_train_count = count_vect.transform(X_train['clean_title'] + \" \" + X_train['clean_body'])\n",
    "# X_test_count = count_vect.transform(X_test['clean_title'] + \" \" + X_test['clean_body'])\n",
    "\n",
    "# # Créer le TfidfVectorizer pour 'title'\n",
    "# tfidf_vect = TfidfVectorizer()\n",
    "# tfidf_vect.fit(X_train['clean_title'])\n",
    "\n",
    "# # Transformer 'title' et 'body' des jeux d'entraînement et de test\n",
    "# X_train_tfidf = tfidf_vect.transform(X_train['clean_title'] + \" \" + X_train['clean_body'])\n",
    "# X_test_tfidf = tfidf_vect.transform(X_test['clean_title'] + \" \" + X_test['clean_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b0379-cdb0-4203-9ff5-a560c8bd82c9",
   "metadata": {},
   "source": [
    "### Définir les fonctions de vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dd1fa-7283-4a8d-84bb-5d69e3b8bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_count(X_train, X_test):\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit(X_train['clean_title'] + \" \" + X_train['clean_body'])\n",
    "    X_train_count = count_vect.transform(X_train['clean_title'] + \" \" + X_train['clean_body'])\n",
    "    X_test_count = count_vect.transform(X_test['clean_title'] + \" \" + X_test['clean_body'])\n",
    "    return X_train_count, X_test_count\n",
    "\n",
    "def vectorize_tfidf(X_train, X_test):\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf_vect.fit(X_train['clean_title'] + \" \" + X_train['clean_body'])\n",
    "    X_train_tfidf = tfidf_vect.transform(X_train['clean_title'] + \" \" + X_train['clean_body'])\n",
    "    X_test_tfidf = tfidf_vect.transform(X_test['clean_title'] + \" \" + X_test['clean_body'])\n",
    "    return X_train_tfidf, X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d89dafb-3269-4966-ac56-aed7099e8259",
   "metadata": {},
   "source": [
    "### Définir la fonction pour exécuter l'expérience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b97102-bc34-405a-855a-19d788775874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(vectorization_method, model, X_train, y_train, X_test, y_test, params, method_name):\n",
    "    X_train_vect, X_test_vect = vectorization_method(X_train, X_test)\n",
    "    params['vectorization'] = method_name\n",
    "    \n",
    "    track_classification_experiment(\n",
    "        model=model,\n",
    "        X_train=X_train_vect,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test_vect,\n",
    "        y_test=y_test,\n",
    "        params=params,\n",
    "        class_labels=['clean_title', 'clean_body']  # Remplacez par les labels de votre jeu de données\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7949c4-48c5-431a-bbec-16686da80e50",
   "metadata": {},
   "source": [
    "### Executer les 2 experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a1807-294e-4b39-b8ec-c80810d7c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les paramètres du modèle\n",
    "params = {\n",
    "    \"C\": 1.0,\n",
    "    \"penalty\": \"l2\",\n",
    "    \"solver\": \"liblinear\"\n",
    "}\n",
    "\n",
    "# Définir le modèle à utiliser (par exemple, la régression logistique)\n",
    "model = LogisticRegression(C=params[\"C\"], penalty=params[\"penalty\"], solver=params[\"solver\"])\n",
    "\n",
    "# Exécuter l'expérience pour CountVectorizer\n",
    "run_experiment(vectorization_method=vectorize_count, model=model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, params=params, method_name=\"CountVectorizer\")\n",
    "\n",
    "# Exécuter l'expérience pour TfidfVectorizer\n",
    "run_experiment(vectorization_method=vectorize_tfidf, model=model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, params=params, method_name=\"TfidfVectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5b00b-c560-4800-a07e-8186f7369bb8",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c735355d-e840-4727-b55d-a321f5380b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Préparer les phrases pour Word2Vec\n",
    "# sentences = [text.split() for text in X_train['clean_title'] + \" \" + X_train['clean_body']]\n",
    "# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Fonction pour obtenir la moyenne des embeddings Word2Vec pour un texte\n",
    "# def get_w2v_features(text, model):\n",
    "#     words = text.split()\n",
    "#     word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "#     return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(model.vector_size)\n",
    "\n",
    "# # Appliquer aux jeux d'entraînement et de test\n",
    "# X_train_w2v = np.array([get_w2v_features(text, word2vec_model) for text in X_train['clean_title'] + \" \" + X_train['clean_body']])\n",
    "# X_test_w2v = np.array([get_w2v_features(text, word2vec_model) for text in X_test['clean_title'] + \" \" + X_test['clean_body']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf83d20-f5f4-4084-9414-218b110d22ce",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e6a48-2c61-4e62-96ca-ac9a87c60696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialiser le tokenizer et le modèle BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Fonction pour obtenir les embeddings BERT\n",
    "# def get_bert_features(text, tokenizer, model):\n",
    "#     inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# # Appliquer aux jeux d'entraînement et de test\n",
    "# X_train_bert = np.array([get_bert_features(text, tokenizer, bert_model).flatten() for text in X_train['clean_title'] + \" \" + X_train['clean_body']])\n",
    "# X_test_bert = np.array([get_bert_features(text, tokenizer, bert_model).flatten() for text in X_test['clean_title'] + \" \" + X_test['clean_body']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0c133-9b23-46d9-9586-dd9e98211f13",
   "metadata": {},
   "source": [
    "### USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78727f44-cfbb-4502-a127-ac7f4cfc1bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maudt\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maudt\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maudt\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maudt\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Charger le modèle USE\n",
    "# use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# # Fonction pour obtenir les embeddings USE\n",
    "# def get_use_features(text, model):\n",
    "#     return model([text]).numpy().flatten()\n",
    "\n",
    "# # Appliquer aux jeux d'entraînement et de test\n",
    "# X_train_use = np.array([get_use_features(text, use_model) for text in X_train['clean_title'] + \" \" + X_train['clean_body']])\n",
    "# X_test_use = np.array([get_use_features(text, use_model) for text in X_test['clean_title'] + \" \" + X_test['clean_body']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
